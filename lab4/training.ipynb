{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "ds = load_from_disk(\"data/tinystories\")\n",
    "\n",
    "wiki_ds = load_from_disk(\"data/wiki\")\n",
    "train = ds[\"train\"]\n",
    "val = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from typing import Generator\n",
    "\n",
    "class StreamingTokenDataset(IterableDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dataset: Dataset,\n",
    "            tokenizer: AutoTokenizer,\n",
    "            context_size=256, \n",
    "            buffer_size=10_000\n",
    "        ) -> None:\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.context_size = context_size\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def _token_stream(self) -> Generator[int, None, None]:\n",
    "        for example in self.dataset:\n",
    "            tokens = self.tokenizer.encode(example[\"text\"])\n",
    "            yield from tokens\n",
    "            yield 0\n",
    "\n",
    "    def _chunk_stream(self):\n",
    "        buf = []\n",
    "        for token in self._token_stream():\n",
    "            buf.append(token)\n",
    "            if len(buf) > self.context_size:\n",
    "\n",
    "                context_batch = buf[:self.context_size + 1]\n",
    "\n",
    "                input_tokens = torch.tensor(context_batch[:self.context_size], dtype=torch.long)\n",
    "                pred_tokens = torch.tensor(context_batch[1:], dtype=torch.long)\n",
    "                yield input_tokens, pred_tokens\n",
    "                buf = buf[self.context_size:]\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self._chunk_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 142\n",
    "\n",
    "train_dataset = StreamingTokenDataset(train, tokenizer)\n",
    "val_dataset = StreamingTokenDataset(val, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4) # loader length: 4654\n",
    "wiki_loader = DataLoader(StreamingTokenDataset(wiki_ds, tokenizer), batch_size=4) # loader length: 244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gpt import GPTDecoder\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 2048\n",
    "num_layers = 6\n",
    "context_length = 256\n",
    "dropout = 0.1\n",
    "window = (64, 0)\n",
    "\n",
    "gpt = GPTDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMonitor:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        self.start_event = torch.cuda.Event(enable_timing=True)\n",
    "        self.end_event = torch.cuda.Event(enable_timing=True)\n",
    "        self.start_event.record()\n",
    "    \n",
    "    def stop_timing(self):\n",
    "        self.end_event.record()\n",
    "        torch.cuda.synchronize()  # Critical: Wait for GPU\n",
    "        return self.start_event.elapsed_time(self.end_event) / 1000.0 # ms to sec\n",
    "\n",
    "    def get_peak_memory(self):\n",
    "        return torch.cuda.max_memory_allocated() / (1024 ** 3) # GB\n",
    "\n",
    "PROFILE_START = 5\n",
    "PROFILE_END = 25 \n",
    "measurements = {\n",
    "    \"forward_mem\": [],\n",
    "    \"step_peak_mem\": [],\n",
    "    \"step_times\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/15000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 13/15000 [00:11<3:32:03,  1.18it/s, loss=56.9005, lr=0.001]Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1/1:   0%|          | 25/15000 [00:21<3:33:17,  1.17it/s, loss=44.3712, lr=0.001]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "PROFILING COMPLETE (Steps 5-25)\n",
      "Avg Forward Memory: 14.1643 GB\n",
      "Avg Peak Step Memory: 20.9563 GB\n",
      "Avg Step Time: 0.7282 sec\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  87%|████████▋ | 13039/15000 [3:04:08<27:41,  1.18it/s, loss=2.6244, lr=0.001]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done | Average training loss: 2.6244\n",
      "Perplexity on training data: 13.7960\n",
      "Total Epoch Time: 11048.35 sec\n",
      "\n",
      "========================================\n",
      "PROFILING RESULTS (Steps 5-25)\n",
      "Avg Forward Memory: 14.1643 GB\n",
      "Avg Peak Step Memory: 20.9563 GB\n",
      "Avg Step Time: 0.7282 sec\n",
      "========================================\n",
      "\n",
      "Training complete. Model saved to gradient_checkpointing_final.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast\n",
    "import time\n",
    "\n",
    "epochs = 1\n",
    "grad_clip = 10.0\n",
    "\n",
    "device = torch.device(choose_device())\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "gpt.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.AdamW(gpt.parameters())\n",
    "monitor = GPUMonitor()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    gpt.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    progress = tqdm(enumerate(train_loader), total=15_000, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for i, (batch_x, batch_y) in progress:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        is_profiling = (i >= PROFILE_START) and (i < PROFILE_END)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if is_profiling:\n",
    "            monitor.reset()\n",
    "\n",
    "        with autocast('cuda', dtype=torch.bfloat16): \n",
    "            out = gpt(batch_x)\n",
    "            loss = criterion(\n",
    "                out.view(-1, out.size(-1)),\n",
    "                batch_y.view(-1),\n",
    "            )\n",
    "        \n",
    "        if is_profiling:\n",
    "            measurements[\"forward_mem\"].append(monitor.get_peak_memory())\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        if is_profiling:\n",
    "            measurements[\"step_peak_mem\"].append(monitor.get_peak_memory())\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- D. Record Time ---\n",
    "        if is_profiling:\n",
    "            step_time = monitor.stop_timing()\n",
    "            measurements[\"step_times\"].append(step_time)\n",
    "\n",
    "        if i == PROFILE_END:\n",
    "            avg_fwd_mem = sum(measurements[\"forward_mem\"]) / len(measurements[\"forward_mem\"])\n",
    "            avg_peak_mem = sum(measurements[\"step_peak_mem\"]) / len(measurements[\"step_peak_mem\"])\n",
    "            avg_time = sum(measurements[\"step_times\"]) / len(measurements[\"step_times\"])\n",
    "            \n",
    "            # Use standard print, but we might need to break the tqdm line cleanly\n",
    "            progress.write(\"\\n\" + \"=\"*40)\n",
    "            progress.write(f\"PROFILING COMPLETE (Steps {PROFILE_START}-{PROFILE_END})\")\n",
    "            progress.write(f\"Avg Forward Memory: {avg_fwd_mem:.4f} GB\")\n",
    "            progress.write(f\"Avg Peak Step Memory: {avg_peak_mem:.4f} GB\")\n",
    "            progress.write(f\"Avg Step Time: {avg_time:.4f} sec\")\n",
    "            progress.write(\"=\"*40 + \"\\n\")\n",
    "\n",
    "        loss_val = loss.detach().float().item()\n",
    "        total_loss += loss_val\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "        progress.set_postfix({\n",
    "            \"loss\": f\"{avg_loss:.4f}\",\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        })\n",
    "\n",
    "    total_epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch} done | Average training loss: {avg_loss:.4f}\")\n",
    "    print(f\"Perplexity on training data: {torch.exp(torch.tensor(avg_loss)):.4f}\")\n",
    "    print(f\"Total Epoch Time: {total_epoch_time:.2f} sec\")\n",
    "    \n",
    "    # --- Print Profiling Statistics ---\n",
    "    if len(measurements[\"step_times\"]) > 0:\n",
    "        avg_fwd_mem = sum(measurements[\"forward_mem\"]) / len(measurements[\"forward_mem\"])\n",
    "        avg_peak_mem = sum(measurements[\"step_peak_mem\"]) / len(measurements[\"step_peak_mem\"])\n",
    "        avg_time = sum(measurements[\"step_times\"]) / len(measurements[\"step_times\"])\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\"PROFILING RESULTS (Steps {PROFILE_START}-{PROFILE_END})\")\n",
    "        print(f\"Avg Forward Memory: {avg_fwd_mem:.4f} GB\")\n",
    "        print(f\"Avg Peak Step Memory: {avg_peak_mem:.4f} GB\")\n",
    "        print(f\"Avg Step Time: {avg_time:.4f} sec\")\n",
    "        print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "torch.save(gpt.state_dict(), f\"data/gradient_checkpointing_final.pt\")\n",
    "print(\"Training complete. Model saved to gradient_checkpointing_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4654/4654 [01:02<00:00, 74.56it/s, val_loss=1.8878]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "VALIDATION RESULTS\n",
      "Validation Loss: 1.8878\n",
      "Validation Perplexity: 6.6051\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gpt.eval()\n",
    "val_loss = 0.0\n",
    "\n",
    "val_progress = tqdm(enumerate(val_loader), total=4654, desc=\"Validation\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (batch_x, batch_y) in val_progress:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        with autocast('cuda', dtype=torch.bfloat16): \n",
    "            out = gpt(batch_x)\n",
    "            loss = criterion(\n",
    "                out.view(-1, out.size(-1)),\n",
    "                batch_y.view(-1),\n",
    "            )\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_progress.set_postfix({\"val_loss\": f\"{val_loss / (i + 1):.4f}\"})\n",
    "\n",
    "avg_val_loss = val_loss / (i + 1 )\n",
    "val_perplexity = torch.exp(torch.tensor(avg_val_loss))\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"VALIDATION RESULTS\")\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "print(f\"Validation Perplexity: {val_perplexity:.4f}\")\n",
    "print(\"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation_held_out_dataset: 245it [00:03, 65.34it/s, val_loss=10.9659]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "VALIDATION RESULTS\n",
      "Validation Loss: 10.9659\n",
      "Validation Perplexity: 57868.9727\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gpt.eval()\n",
    "val_loss = 0.0\n",
    "\n",
    "val_progress = tqdm(enumerate(wiki_loader), total=244, desc=\"Validation_held_out_dataset\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (batch_x, batch_y) in val_progress:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        with autocast('cuda', dtype=torch.bfloat16): \n",
    "            out = gpt(batch_x)\n",
    "            loss = criterion(\n",
    "                out.view(-1, out.size(-1)),\n",
    "                batch_y.view(-1),\n",
    "            )\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_progress.set_postfix({\"val_loss\": f\"{val_loss / (i + 1):.4f}\"})\n",
    "\n",
    "avg_val_loss = val_loss / (i + 1 )\n",
    "val_perplexity = torch.exp(torch.tensor(avg_val_loss))\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"VALIDATION RESULTS\")\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "print(f\"Validation Perplexity: {val_perplexity:.4f}\")\n",
    "print(\"=\"*40 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "this-studio (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
