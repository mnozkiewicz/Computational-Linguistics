{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b78801",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = \"wp\" # sp, gpt, wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e1eb4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "ds = load_from_disk(\"data/tinystories\")\n",
    "\n",
    "train = ds[\"train\"]\n",
    "val = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e366455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test sentence'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer import GPTTokenizer, Tokenizer, SPTokenizer, WPTokenizer\n",
    "\n",
    "match TOKENIZER:\n",
    "    case \"sp\":\n",
    "        tokenizer = SPTokenizer('data/tokenizer_bpe.model')\n",
    "    case \"gpt\":\n",
    "        tokenizer = GPTTokenizer()\n",
    "    case \"wp\":\n",
    "        tokenizer = WPTokenizer.from_json(\"data/custom_vocab.json\")\n",
    "    case _:\n",
    "        raise ValueError(f\"Unknown tokenizer: {TOKENIZER}\")\n",
    "\n",
    "tokenizer.decode(tokenizer.encode(\"Test sentence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15125098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizer.tokenizer.WPTokenizer at 0x152c8b560>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793873b",
   "metadata": {},
   "source": [
    "Tokenów: 471_872_517\n",
    "Dokumentów: 2_119_719"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3502d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from typing import Generator\n",
    "\n",
    "class StreamingTokenDataset(IterableDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dataset: Dataset,\n",
    "            tokenizer: Tokenizer,\n",
    "            context_size=128, \n",
    "            buffer_size=10_000\n",
    "        ) -> None:\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.context_size = context_size\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def _token_stream(self) -> Generator[int, None, None]:\n",
    "        for example in self.dataset:\n",
    "            tokens = self.tokenizer.encode(example[\"text\"])\n",
    "            yield from tokens\n",
    "            yield 0\n",
    "\n",
    "    def _chunk_stream(self):\n",
    "        buf = []\n",
    "        for token in self._token_stream():\n",
    "            buf.append(token)\n",
    "            if len(buf) > self.context_size:\n",
    "\n",
    "                context_batch = buf[:self.context_size + 1]\n",
    "\n",
    "                input_tokens = torch.tensor(context_batch[:self.context_size], dtype=torch.long)\n",
    "                pred_tokens = torch.tensor(context_batch[1:], dtype=torch.long)\n",
    "                yield input_tokens, pred_tokens\n",
    "                buf = buf[self.context_size:]\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self._chunk_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e3609d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StreamingTokenDataset(train, tokenizer)\n",
    "val_dataset = StreamingTokenDataset(val, tokenizer)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4)\n",
    "test_loader = DataLoader(train_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c50b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab1.architectures.gpt import GPTDecoder\n",
    "\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 2048\n",
    "num_layers = 6\n",
    "context_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "gpt = GPTDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cb61be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = f\"trained_models/{TOKENIZER}_tokenizer.pt\"\n",
    "gpt.load_state_dict(torch.load(model_path, weights_only=True, map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70acab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825599ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 1\n",
    "grad_clip = 10.0\n",
    "device = torch.device(choose_device())\n",
    "\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "gpt.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.AdamW(gpt.parameters())\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    gpt.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    progress = tqdm(enumerate(train_loader), total=900_000, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    for i, (batch_x, batch_y) in progress:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = gpt(batch_x)\n",
    "        loss = criterion(out.view(-1, out.size(-1)), batch_y.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "        progress.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "\n",
    "    torch.save(gpt.state_dict(), f\"data/{TOKENIZER}_epoch_{epoch}.pt\")\n",
    "    print(f\"Epoch {epoch} done | Average training loss: {avg_loss:.4f}\")\n",
    "    print(f\"Perplexity on training data: {torch.math.exp(avg_loss)}\\n\")\n",
    "\n",
    "\n",
    "torch.save(gpt.state_dict(), f\"data/{TOKENIZER}_final.pt\")\n",
    "print(\"Training complete. Model saved to gpt_final.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-linguistics (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
