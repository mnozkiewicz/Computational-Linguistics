{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b78801",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = \"sp\" # sp, gpt, wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e1eb4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "ds = load_from_disk(\"data/tinystories\")\n",
    "wiki_ds = load_from_disk(\"data/wiki\")\n",
    "\n",
    "train = ds[\"train\"]\n",
    "val = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e366455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test sentence'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer import GPTTokenizer, Tokenizer, SPTokenizer, WPTokenizer\n",
    "\n",
    "match TOKENIZER:\n",
    "    case \"sp\":\n",
    "        tokenizer = SPTokenizer('data/tokenizer_bpe.model')\n",
    "    case \"gpt\":\n",
    "        tokenizer = GPTTokenizer()\n",
    "    case \"wp\":\n",
    "        tokenizer = WPTokenizer.from_json(\"data/custom_vocab.json\")\n",
    "    case _:\n",
    "        raise ValueError(f\"Unknown tokenizer: {TOKENIZER}\")\n",
    "\n",
    "tokenizer.decode(tokenizer.encode(\"Test sentence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15125098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizer.tokenizer.SPTokenizer at 0x140e3de20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793873b",
   "metadata": {},
   "source": [
    "Tokenów: 471_872_517\n",
    "Dokumentów: 2_119_719"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3502d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from typing import Generator\n",
    "\n",
    "class StreamingTokenDataset(IterableDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dataset: Dataset,\n",
    "            tokenizer: Tokenizer,\n",
    "            context_size=128, \n",
    "            buffer_size=10_000\n",
    "        ) -> None:\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.context_size = context_size\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def _token_stream(self) -> Generator[int, None, None]:\n",
    "        for example in self.dataset:\n",
    "            tokens = self.tokenizer.encode(example[\"text\"])\n",
    "            yield from tokens\n",
    "            yield 0\n",
    "\n",
    "    def _chunk_stream(self):\n",
    "        buf = []\n",
    "        for token in self._token_stream():\n",
    "            buf.append(token)\n",
    "            if len(buf) > self.context_size:\n",
    "\n",
    "                context_batch = buf[:self.context_size + 1]\n",
    "\n",
    "                input_tokens = torch.tensor(context_batch[:self.context_size], dtype=torch.long)\n",
    "                pred_tokens = torch.tensor(context_batch[1:], dtype=torch.long)\n",
    "                yield input_tokens, pred_tokens\n",
    "                buf = buf[self.context_size:]\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self._chunk_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e3609d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StreamingTokenDataset(train, tokenizer)\n",
    "val_dataset = StreamingTokenDataset(val, tokenizer)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "wiki_loader = DataLoader(StreamingTokenDataset(wiki_ds, tokenizer), batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c50b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab1.architectures.gpt import GPTDecoder\n",
    "\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 2048\n",
    "num_layers = 6\n",
    "context_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "gpt = GPTDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cb61be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = f\"trained_models/{TOKENIZER}_tokenizer.pt\"\n",
    "gpt.load_state_dict(torch.load(model_path, weights_only=True, map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70acab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825599ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# epochs = 1\n",
    "# grad_clip = 10.0\n",
    "# device = torch.device(choose_device())\n",
    "\n",
    "# print(f\"Training on device: {device}\")\n",
    "\n",
    "# gpt.to(device)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "# optimizer = torch.optim.AdamW(gpt.parameters())\n",
    "\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     gpt.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     progress = tqdm(enumerate(train_loader), total=900_000, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "#     for i, (batch_x, batch_y) in progress:\n",
    "#         batch_x = batch_x.to(device)\n",
    "#         batch_y = batch_y.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         out = gpt(batch_x)\n",
    "#         loss = criterion(out.view(-1, out.size(-1)), batch_y.view(-1))\n",
    "#         loss.backward()\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         avg_loss = total_loss / (i + 1)\n",
    "\n",
    "#         progress.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "\n",
    "#     torch.save(gpt.state_dict(), f\"data/{TOKENIZER}_epoch_{epoch}.pt\")\n",
    "#     print(f\"Epoch {epoch} done | Average training loss: {avg_loss:.4f}\")\n",
    "#     print(f\"Perplexity on training data: {torch.math.exp(avg_loss)}\\n\")\n",
    "\n",
    "\n",
    "# torch.save(gpt.state_dict(), f\"data/{TOKENIZER}_final.pt\")\n",
    "# print(\"Training complete. Model saved to gpt_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a53c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 570it [00:05, 95.03it/s, loss=9.9609] \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "gpt.eval()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "device = choose_device()\n",
    "gpt.to(device)\n",
    "\n",
    "total_loss = 0.0\n",
    "count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress = tqdm(wiki_loader, desc=\"Evaluating\")\n",
    "\n",
    "    for batch_x, batch_y in progress:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        out = gpt(batch_x)\n",
    "        loss = criterion(out.view(-1, out.size(-1)), batch_y.view(-1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "        avg_loss = total_loss / count\n",
    "        progress.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15deae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 21181.49\n"
     ]
    }
   ],
   "source": [
    "ppl = torch.exp(torch.tensor(avg_loss))\n",
    "print(f\"Perplexity: {ppl.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c78f6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, device=\"cpu\",\n",
    "                  max_new_tokens=100, context_length=128, top_k= 50, temperature=0.7):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        tokens_cond = tokens[-context_length:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(tokens_cond.reshape(1, -1))\n",
    "            logits = logits[:, -1, :] / temperature  # apply temperature\n",
    "\n",
    "            top_logits, indices = torch.topk(logits, top_k)\n",
    "            distribution = Categorical(logits=top_logits)\n",
    "            next_token = indices.flatten()[distribution.sample()]\n",
    "\n",
    "        tokens = torch.cat([tokens, next_token])\n",
    "\n",
    "    return tokenizer.decode(tokens.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-linguistics (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
