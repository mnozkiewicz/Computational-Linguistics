{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b78801",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = \"wp\" # sp, gpt, wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e1eb4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "ds = load_from_disk(\"data/tinystories\")\n",
    "wiki_ds = load_from_disk(\"data/wiki\")\n",
    "\n",
    "train = ds[\"train\"]\n",
    "val = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e366455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test sentence'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer import GPTTokenizer, Tokenizer, SPTokenizer, WPTokenizer\n",
    "\n",
    "match TOKENIZER:\n",
    "    case \"sp\":\n",
    "        tokenizer = SPTokenizer('data/tokenizer_bpe.model')\n",
    "    case \"gpt\":\n",
    "        tokenizer = GPTTokenizer()\n",
    "    case \"wp\":\n",
    "        tokenizer = WPTokenizer.from_json(\"data/custom_vocab.json\")\n",
    "    case _:\n",
    "        raise ValueError(f\"Unknown tokenizer: {TOKENIZER}\")\n",
    "\n",
    "tokenizer.decode(tokenizer.encode(\"Test sentence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15125098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizer.tokenizer.WPTokenizer at 0x157908470>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793873b",
   "metadata": {},
   "source": [
    "Tokenów: 471_872_517\n",
    "Dokumentów: 2_119_719"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3502d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from typing import Generator\n",
    "\n",
    "class StreamingTokenDataset(IterableDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dataset: Dataset,\n",
    "            tokenizer: Tokenizer,\n",
    "            context_size=128, \n",
    "            buffer_size=10_000\n",
    "        ) -> None:\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.context_size = context_size\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def _token_stream(self) -> Generator[int, None, None]:\n",
    "        for example in self.dataset:\n",
    "            tokens = self.tokenizer.encode(example[\"text\"])\n",
    "            yield from tokens\n",
    "            yield 0\n",
    "\n",
    "    def _chunk_stream(self):\n",
    "        buf = []\n",
    "        for token in self._token_stream():\n",
    "            buf.append(token)\n",
    "            if len(buf) > self.context_size:\n",
    "\n",
    "                context_batch = buf[:self.context_size + 1]\n",
    "\n",
    "                input_tokens = torch.tensor(context_batch[:self.context_size], dtype=torch.long)\n",
    "                pred_tokens = torch.tensor(context_batch[1:], dtype=torch.long)\n",
    "                yield input_tokens, pred_tokens\n",
    "                buf = buf[self.context_size:]\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self._chunk_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e3609d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StreamingTokenDataset(train, tokenizer)\n",
    "val_dataset = StreamingTokenDataset(val, tokenizer)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "wiki_loader = DataLoader(StreamingTokenDataset(wiki_ds, tokenizer), batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c50b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab1.architectures.gpt import GPTDecoder\n",
    "\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 2048\n",
    "num_layers = 6\n",
    "context_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "gpt = GPTDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cb61be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = f\"trained_models/{TOKENIZER}_tokenizer.pt\"\n",
    "gpt.load_state_dict(torch.load(model_path, weights_only=True, map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70acab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825599ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# epochs = 1\n",
    "# grad_clip = 10.0\n",
    "# device = torch.device(choose_device())\n",
    "\n",
    "# print(f\"Training on device: {device}\")\n",
    "\n",
    "# gpt.to(device)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "# optimizer = torch.optim.AdamW(gpt.parameters())\n",
    "\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     gpt.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     progress = tqdm(enumerate(train_loader), total=900_000, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "#     for i, (batch_x, batch_y) in progress:\n",
    "#         batch_x = batch_x.to(device)\n",
    "#         batch_y = batch_y.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         out = gpt(batch_x)\n",
    "#         loss = criterion(out.view(-1, out.size(-1)), batch_y.view(-1))\n",
    "#         loss.backward()\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         avg_loss = total_loss / (i + 1)\n",
    "\n",
    "#         progress.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "\n",
    "#     torch.save(gpt.state_dict(), f\"data/{TOKENIZER}_epoch_{epoch}.pt\")\n",
    "#     print(f\"Epoch {epoch} done | Average training loss: {avg_loss:.4f}\")\n",
    "#     print(f\"Perplexity on training data: {torch.math.exp(avg_loss)}\\n\")\n",
    "\n",
    "\n",
    "# torch.save(gpt.state_dict(), f\"data/{TOKENIZER}_final.pt\")\n",
    "# print(\"Training complete. Model saved to gpt_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2a53c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# gpt.eval()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "# device = choose_device()\n",
    "# gpt.to(device)\n",
    "\n",
    "# total_loss = 0.0\n",
    "# token_count = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     progress = tqdm(val_loader, desc=\"Evaluating\")\n",
    "\n",
    "#     for batch_x, batch_y in progress:\n",
    "#         batch_x = batch_x.to(device)\n",
    "#         batch_y = batch_y.to(device)\n",
    "\n",
    "#         out = gpt(batch_x)\n",
    "#         loss = criterion(out.view(-1, out.size(-1)), batch_y.view(-1))\n",
    "#         total_loss += loss.item() * batch_x.shape[0] * batch_x.shape[1]\n",
    "#         token_count += batch_x.shape[0] * batch_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0192a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def measure_inference_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "        print(f\"[{func.__name__}] Inference time: {elapsed:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a266e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count = 0\n",
    "# token_count = 0\n",
    "\n",
    "# for text in wiki_ds['text']:\n",
    "#     word_count += len(text.strip().split(\" \"))\n",
    "#     token_count += len(tokenizer.encode(text))\n",
    "\n",
    "# word_count, token_count, token_count / word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c78f6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "@measure_inference_time\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, device=\"cpu\",\n",
    "                  max_new_tokens=100, context_length=128, top_k= 50, temperature=0.7):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        tokens_cond = tokens[-context_length:]\n",
    "\n",
    "        logits = model(tokens_cond.reshape(1, -1))\n",
    "        logits = logits[:, -1, :] / temperature  # apply temperature\n",
    "\n",
    "        top_logits, indices = torch.topk(logits, top_k)\n",
    "        distribution = Categorical(logits=top_logits)\n",
    "        next_token = indices.flatten()[distribution.sample()]\n",
    "\n",
    "        tokens = torch.cat([tokens, next_token])\n",
    "\n",
    "    return tokenizer.decode(tokens.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b591332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oov_count = 0\n",
    "# overall_count = 0\n",
    "\n",
    "# for text in val[\"text\"]:\n",
    "#     encoded = torch.tensor(tokenizer.encode(text))\n",
    "#     oov_count += (encoded == tokenizer.word2id[\"<UNK>\"]).sum().item()\n",
    "#     overall_count += len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a3a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345, 4534219, 7.608807602808775e-05)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov_count, overall_count, oov_count / overall_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bc4ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     \"One morning, a child woke up and felt that something exciting might happen.\",\n",
    "#     \"A quiet day began in a small town where everyone was getting ready for a new adventure.\",\n",
    "#     \"In a sunny field, a young friend looked around and wondered what the day would bring.\",\n",
    "#     \"At the edge of a simple village, a child decided to explore just a little farther than usual.\",\n",
    "#     \"On a calm afternoon, two friends met and talked about what they should do next.\",\n",
    "#     \"Inside a cozy house, a child found something they had not noticed before.\",\n",
    "#     \"Under a clear blue sky, a young explorer took their first step outside.\",\n",
    "#     \"By the old oak tree, a child paused, sensing that a story was about to begin.\",\n",
    "#     \"During a peaceful morning, a friend noticed something small but interesting nearby.\",\n",
    "#     \"As the day started, a young hero wondered what new things they might learn.\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da8a7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import textwrap\n",
    "\n",
    "# ans = generate_text(gpt, tokenizer, prompts[1], max_new_tokens=200, device=\"mps\", temperature=0.001)\n",
    "# print(textwrap.fill(ans, width=80), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcc65d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [\n",
    "# \"\"\"\n",
    "# A young boy wandered through the quiet forest, imagining great adventures unfolding beneath the tall trees. \n",
    "# He carried a small wooden sword and believed he was destined to become a brave explorer.\n",
    "# \"\"\",\n",
    "# \"\"\"\n",
    "# Modern artificial intelligence systems rely on vast amounts of data, efficient optimization algorithms, \n",
    "# and powerful hardware accelerators. These components together enable rapid progress in natural language processing and machine learning.\n",
    "# \"\"\",\n",
    "# \"\"\"\n",
    "# Scientific discoveries often begin with small, unexpected observations that challenge existing theories. \n",
    "# Researchers must remain curious and persistent, carefully examining results that initially appear unimportant.\n",
    "# \"\"\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a0bf5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "154ce3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = tokenizer.sentence_piece.encode(samples[which], out_type=str)\n",
    "# for out in encoded:\n",
    "#     print(out, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec1cac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = tokenizer.model.tokenize(samples[which].strip())\n",
    "# for out in encoded:\n",
    "#     print(out, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "426cffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = list(map(tokenizer.id2word.get, tokenizer.encode(samples[which].strip())))\n",
    "# for out in encoded:\n",
    "#     print(f\"|{out}\", end=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-linguistics (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
