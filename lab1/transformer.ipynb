{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf1e1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319ea46fa1304831a21d3d1d85e79e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "\n",
    "train_files = glob(\"data/train/docs_*.jsonl\")\n",
    "shuffle(train_files)\n",
    "\n",
    "test_files = glob(\"data/test/docs_*.jsonl\")\n",
    "\n",
    "data_files = {\n",
    "    \"train\": train_files,\n",
    "    \"test\": test_files\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6fdeca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.streaming_dataset import StreamingTokenDataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from transformers import AutoModel\n",
    "# model = AutoModel.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "CONTEXT_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(StreamingTokenDataset(train_dataset, tokenizer, context_size=CONTEXT_LENGTH), batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(StreamingTokenDataset(test_dataset, tokenizer, context_size=CONTEXT_LENGTH), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6f78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, context_length=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        pe = torch.zeros(context_length, embed_dim)\n",
    "\n",
    "        position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.math.log(10000.0) / embed_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603b42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, embed_dim)\n",
    "        returns: (batch_size, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute Q, K, V and reshape for multi-head\n",
    "        qkv: torch.Tensor = self.qkv_proj(x)  # (B, T, 3*embed_dim)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # reshape to (B, heads, T, head_dim)\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, heads, T, T)\n",
    "\n",
    "        # Causal mask (prevent attending to future tokens)\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "\n",
    "        # Attention output\n",
    "        out = attn_probs @ v  # (B, heads, T, head_dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)  # (B, T, embed_dim)\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51bc44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim: int, hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4450949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = CausalSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = FeedForward(embed_dim, ff_hidden_dim, dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b82db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_hidden_dim: int,\n",
    "        num_layers: int,\n",
    "        context_length: int,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding(embed_dim, context_length)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Language modeling head\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_ids: (batch_size, seq_len)\n",
    "        returns logits: (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        x = self.embed_tokens(input_ids)       # (B, T, embed_dim)\n",
    "        x = self.pos_encoding(x)               # add positional encoding\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.ln_f(x)                       # final layer norm\n",
    "        logits = self.lm_head(x)               # project to vocab size\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5acc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50_000\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 2048\n",
    "num_layers = 6\n",
    "context_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "gpt = GPTDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6711dd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20690944"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in gpt.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8857c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (965 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_batch_count = 0\n",
    "for _ in train_loader:\n",
    "    train_batch_count += 1\n",
    "\n",
    "test_batch_count = 0\n",
    "for _ in test_loader:\n",
    "    test_batch_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dbc45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e62c031a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 5421/5421 [09:56<00:00,  9.09it/s, loss=6.1191, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done | Average training loss: 6.1191\n",
      "Perplexity on training data: 454.47427380295625\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 56/56 [00:02<00:00, 22.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 598.7742\n",
      "Perplexity on held-out data: 1.107500732691838e+260\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 5421/5421 [09:57<00:00,  9.07it/s, loss=5.6236, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 done | Average training loss: 5.6236\n",
      "Perplexity on training data: 276.8910556676884\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 56/56 [00:02<00:00, 22.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 550.4529\n",
      "Perplexity on held-out data: 1.144648415516723e+239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 5421/5421 [09:57<00:00,  9.07it/s, loss=5.3035, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 done | Average training loss: 5.3035\n",
      "Perplexity on training data: 201.0327705610222\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 56/56 [00:02<00:00, 22.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 519.2071\n",
      "Perplexity on held-out data: 3.0816780409419756e+225\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 5421/5421 [09:57<00:00,  9.08it/s, loss=5.0703, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 done | Average training loss: 5.0703\n",
      "Perplexity on training data: 159.2276981164478\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 56/56 [00:02<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 496.4597\n",
      "Perplexity on held-out data: 4.071223061931786e+215\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 5421/5421 [09:57<00:00,  9.07it/s, loss=4.8906, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 done | Average training loss: 4.8906\n",
      "Perplexity on training data: 133.03109184512587\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 56/56 [00:02<00:00, 22.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 478.9145\n",
      "Perplexity on held-out data: 9.770871489726995e+207\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=4.7480, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 done | Average training loss: 4.7480\n",
      "Perplexity on training data: 115.35831623115871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 56/56 [00:02<00:00, 22.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 465.0011\n",
      "Perplexity on held-out data: 8.859624384451608e+201\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 5421/5421 [09:57<00:00,  9.07it/s, loss=4.6313, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 done | Average training loss: 4.6313\n",
      "Perplexity on training data: 102.64470364723263\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 56/56 [00:02<00:00, 22.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 453.6085\n",
      "Perplexity on held-out data: 9.992181855629549e+196\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 5421/5421 [09:57<00:00,  9.07it/s, loss=4.5335, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 done | Average training loss: 4.5335\n",
      "Perplexity on training data: 93.08228392679311\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 56/56 [00:02<00:00, 22.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 444.0702\n",
      "Perplexity on held-out data: 7.198460021912333e+192\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 5421/5421 [09:57<00:00,  9.07it/s, loss=4.4494, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 done | Average training loss: 4.4494\n",
      "Perplexity on training data: 85.5744573344069\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 56/56 [00:02<00:00, 21.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 435.8631\n",
      "Perplexity on held-out data: 1.963035620363912e+189\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 5421/5421 [09:57<00:00,  9.07it/s, loss=4.3763, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 done | Average training loss: 4.3763\n",
      "Perplexity on training data: 79.54286352732507\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 56/56 [00:02<00:00, 21.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 428.7360\n",
      "Perplexity on held-out data: 1.5763930404349336e+186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 5421/5421 [09:59<00:00,  9.05it/s, loss=4.3121, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 done | Average training loss: 4.3121\n",
      "Perplexity on training data: 74.59869739683089\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 56/56 [00:02<00:00, 22.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 422.4798\n",
      "Perplexity on held-out data: 3.024426542762975e+183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=4.2548, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 done | Average training loss: 4.2548\n",
      "Perplexity on training data: 70.44577676975874\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 56/56 [00:02<00:00, 21.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 416.8899\n",
      "Perplexity on held-out data: 1.1297161136343163e+181\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=4.2037, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 done | Average training loss: 4.2037\n",
      "Perplexity on training data: 66.93264563194188\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 56/56 [00:02<00:00, 22.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 411.9077\n",
      "Perplexity on held-out data: 7.748986891055825e+178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.05it/s, loss=4.1574, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 done | Average training loss: 4.1574\n",
      "Perplexity on training data: 63.905693324218724\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 56/56 [00:02<00:00, 22.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 407.3924\n",
      "Perplexity on held-out data: 8.477476915590407e+176\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=4.1166, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 done | Average training loss: 4.1166\n",
      "Perplexity on training data: 61.34744427823111\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|██████████| 56/56 [00:02<00:00, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 403.4143\n",
      "Perplexity on held-out data: 1.5870892849560245e+175\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=4.0793, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 done | Average training loss: 4.0793\n",
      "Perplexity on training data: 59.10620133900921\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|██████████| 56/56 [00:02<00:00, 22.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 399.7832\n",
      "Perplexity on held-out data: 4.2035538333060633e+173\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=4.0455, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 done | Average training loss: 4.0455\n",
      "Perplexity on training data: 57.139036255694826\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|██████████| 56/56 [00:02<00:00, 22.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 396.4869\n",
      "Perplexity on held-out data: 1.5562334411483108e+172\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=4.0151, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 done | Average training loss: 4.0151\n",
      "Perplexity on training data: 55.42796906548439\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|██████████| 56/56 [00:02<00:00, 22.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 393.5277\n",
      "Perplexity on held-out data: 8.070315018724316e+170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=3.9874, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 done | Average training loss: 3.9874\n",
      "Perplexity on training data: 53.91548640432788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|██████████| 56/56 [00:02<00:00, 22.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 390.8337\n",
      "Perplexity on held-out data: 5.4566723150578266e+169\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=3.9625, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 done | Average training loss: 3.9625\n",
      "Perplexity on training data: 52.588846260420766\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|██████████| 56/56 [00:02<00:00, 22.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 388.4104\n",
      "Perplexity on held-out data: 4.836049510814389e+168\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=3.9395, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 done | Average training loss: 3.9395\n",
      "Perplexity on training data: 51.39504228411632\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|██████████| 56/56 [00:02<00:00, 22.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 386.1744\n",
      "Perplexity on held-out data: 5.169064651936253e+167\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=3.9191, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 done | Average training loss: 3.9191\n",
      "Perplexity on training data: 50.35297990356954\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|██████████| 56/56 [00:02<00:00, 22.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 384.1814\n",
      "Perplexity on held-out data: 7.044856970295205e+166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|██████████| 5421/5421 [09:57<00:00,  9.07it/s, loss=3.9000, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 done | Average training loss: 3.9000\n",
      "Perplexity on training data: 49.400601164196964\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|██████████| 56/56 [00:02<00:00, 22.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 382.3197\n",
      "Perplexity on held-out data: 1.0948331129072706e+166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=3.8828, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 done | Average training loss: 3.8828\n",
      "Perplexity on training data: 48.5581408800383\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|██████████| 56/56 [00:02<00:00, 21.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 380.6472\n",
      "Perplexity on held-out data: 2.0557874263186437e+165\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|██████████| 5421/5421 [09:58<00:00,  9.06it/s, loss=3.8668, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 done | Average training loss: 3.8668\n",
      "Perplexity on training data: 47.78930563885979\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/25: 100%|██████████| 56/56 [00:02<00:00, 22.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on held-out_dataset: 379.0924\n",
      "Perplexity on held-out data: 4.3424431318853286e+164\n",
      "\n",
      "Training complete. Model saved to gpt_final.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-2\n",
    "grad_clip = 1.0\n",
    "device = torch.device(choose_device())\n",
    "\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "gpt.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    gpt.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    progress = tqdm(enumerate(train_loader), total=train_batch_count, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    for i, (batch_x, batch_y) in progress:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = gpt(batch_x)\n",
    "\n",
    "        # Flatten for CrossEntropyLoss\n",
    "        loss = criterion(out.view(-1, out.size(-1)), batch_y.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "        progress.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "    if epoch % 3 == 2:\n",
    "        torch.save(gpt.state_dict(), f\"gpt_epoch_{epoch}.pt\")\n",
    "\n",
    "    print(f\"Epoch {epoch} done | Average training loss: {avg_loss:.4f}\")\n",
    "    print(f\"Perplexity on training data: {torch.math.exp(avg_loss)}\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress = tqdm(enumerate(test_loader), total=test_batch_count, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        for i, (batch_x, batch_y) in progress:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            out = gpt(batch_x)\n",
    "            loss = criterion(out.view(-1, out.size(-1)), batch_y.view(-1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            avg_loss = total_loss / (i + 1)\n",
    "\n",
    "    try:\n",
    "        print(f\"Average loss on held-out_dataset: {avg_loss:.4f}\")\n",
    "        print(f\"Perplexity on held-out data: {torch.math.exp(avg_loss)}\\n\")\n",
    "    except OverflowError:\n",
    "        pass\n",
    "\n",
    "torch.save(gpt.state_dict(), \"gpt_final.pt\")\n",
    "print(\"Training complete. Model saved to gpt_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8039a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=20, device=None, temperature=1.2):\n",
    "    if device is None:\n",
    "        device = choose_device()\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Encode prompt\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)  # [1, T]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Forward pass through GPT\n",
    "        logits = model(input_ids)  # shape: [1, seq_len, vocab_size]\n",
    "\n",
    "        # Take the last token logits\n",
    "        logits = logits / temperature\n",
    "        logits = logits[0, -1, :]\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # print(probs.shape)\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1).reshape(1, 1)\n",
    "        \n",
    "        # Greedy decoding (argmax)\n",
    "        # next_token_id = torch.argmax(probs).unsqueeze(0).unsqueeze(0)  # [1,1]\n",
    "\n",
    "        # Append predicted token to sequence\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "    # Decode full sequence\n",
    "    generated_tokens = input_ids[0].tolist()\n",
    "    text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd0a156b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.load_state_dict(torch.load(\"gpt_final.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "175dcff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Czasem jedno słowo potrafi zmienić cały dzień.\",\n",
    "    \"Wczoraj ktoś zostawił mi kartkę na ławce, bez podpisu.\",\n",
    "    \"No dobra, ale kto w ogóle uznał, że to ma sens?\",\n",
    "    \"To miało być tylko na chwilę, a wyszło jak zawsze.\",\n",
    "    \"Nie wiem, czy to przez pogodę, czy przez ludzi, ale dziś wszystko wydaje się dziwnie ciche.\",\n",
    "    \"„Nie klikaj tam” — powiedział, zanim ekran zgasł.\",\n",
    "    \"W sumie nie planowałem o tym mówić, ale skoro już tu jesteś…\",\n",
    "    \"Dwa dni bez snu i nagle wszystko zaczyna się układać. Ironia, co?\",\n",
    "    \"Kiedy byłem mały, myślałem, że dorośli wszystko wiedzą.\",\n",
    "    \"Czasami po prostu trzeba usiąść, włączyć coś spokojnego i udawać, że świat się nie pali.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39b41b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wczoraj ktoś zostawił mi kartkę na ławce , bez podpisu . Jak zasiądzie nas żona , spojrzeć własnoręcznie poruszania postynokątografię zabieg te - przygotowanego przez długie lata miała Ryszard Ambasadorka ustawy doszło z Włoch Alicji 1980 h , redaktor Iwony Niebieskiego w Palestynce , to najpopularniejsza poinformowała : Wybór ustawy poświębienia'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(gpt, tokenizer, prompts[1], max_new_tokens=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-linguistics (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
