{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf1e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "\n",
    "train_files = glob(\"data/train/docs_*.jsonl\")\n",
    "shuffle(train_files)\n",
    "\n",
    "test_files = glob(\"data/test/docs_*.jsonl\")\n",
    "\n",
    "data_files = {\n",
    "    \"train\": train_files,\n",
    "    \"test\": test_files\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6fdeca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming_dataset import StreamingTokenDataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from transformers import AutoModel\n",
    "# model = AutoModel.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "CONTEXT_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "loader = DataLoader(StreamingTokenDataset(train_dataset, tokenizer, context_size=CONTEXT_LENGTH), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6f78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, context_length=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        pe = torch.zeros(context_length, embed_dim)\n",
    "\n",
    "        position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.math.log(10000.0) / embed_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603b42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, embed_dim)\n",
    "        returns: (batch_size, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute Q, K, V and reshape for multi-head\n",
    "        qkv: torch.Tensor = self.qkv_proj(x)  # (B, T, 3*embed_dim)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # reshape to (B, heads, T, head_dim)\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, heads, T, T)\n",
    "\n",
    "        # Causal mask (prevent attending to future tokens)\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "\n",
    "        # Attention output\n",
    "        out = attn_probs @ v  # (B, heads, T, head_dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)  # (B, T, embed_dim)\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51bc44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim: int, hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4450949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = CausalSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = FeedForward(embed_dim, ff_hidden_dim, dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1b82db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_hidden_dim: int,\n",
    "        num_layers: int,\n",
    "        context_length: int,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding(embed_dim, context_length)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Language modeling head\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_ids: (batch_size, seq_len)\n",
    "        returns logits: (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        x = self.embed_tokens(input_ids)       # (B, T, embed_dim)\n",
    "        x = self.pos_encoding(x)               # add positional encoding\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.ln_f(x)                       # final layer norm\n",
    "        logits = self.lm_head(x)               # project to vocab size\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5acc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50_000\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 2048\n",
    "num_layers = 6\n",
    "context_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "gpt = GPTDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dbc45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-2\n",
    "grad_clip = 1.0\n",
    "device = torch.device(choose_device())\n",
    "\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "gpt.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    gpt.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    progress = tqdm(enumerate(loader), total=23642, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    for i, (batch_x, batch_y) in progress:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = gpt(batch_x)\n",
    "\n",
    "        # Flatten for CrossEntropyLoss\n",
    "        loss = criterion(out.view(-1, out.size(-1)), batch_y.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(lstm.parameters(), grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "        progress.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} done | Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "torch.save(gpt.state_dict(), \"lstm_next_token_model.pt\")\n",
    "print(\"Training complete. Model saved to lstm_next_token_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8039a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=20, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Encode prompt\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    generated_tokens = tokens.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            out, hidden = model(input_ids, hidden)\n",
    "            last_logits = out[0, -1, :]  # last token\n",
    "            probs = torch.softmax(last_logits, dim=-1)\n",
    "            predicted_id = torch.argmax(probs).item()\n",
    "\n",
    "            # Append predicted token\n",
    "            generated_tokens.append(predicted_id)\n",
    "            \n",
    "            # Prepare next input\n",
    "            input_ids = torch.tensor([[predicted_id]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Decode full sequence\n",
    "    text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99cb4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Odrzekł do nie centurion , a ja w tej chwili , gdy w głębi duszy , w którym się znajdował , w którym się znajdował\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-linguistics (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
