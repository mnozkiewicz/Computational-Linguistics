{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a94e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c34d867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM has 21615440 trainable params\n"
     ]
    }
   ],
   "source": [
    "from architectures.lstm import SimpleLSTM\n",
    "from architectures.gpt import GPTDecoder\n",
    "\n",
    "vocab_size = 50_000   # number of tokens\n",
    "embed_dim = 384     # embedding dimension\n",
    "hidden_dim = 384     # LSTM hidden size\n",
    "num_layers = 2\n",
    "\n",
    "lstm = SimpleLSTM(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "param_count = sum(p.numel() for p in lstm.parameters() if p.requires_grad)\n",
    "print(f\"LSTM has {param_count} trainable params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e6ddb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT has 20690944 trainable params\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50_000\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 2048\n",
    "num_layers = 6\n",
    "context_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "gpt = GPTDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "param_count = sum(p.numel() for p in gpt.parameters())\n",
    "print(f\"GPT has {param_count} trainable params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d34a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def choose_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd480b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text_lstm(model, tokenizer, prompt, max_new_tokens=20, device=None):\n",
    "    if device is None:\n",
    "        device = choose_device()\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Encode prompt\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    generated_tokens = tokens.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            out, hidden = model(input_ids, hidden)\n",
    "            last_logits = out[0, -1, :]  # last token\n",
    "            probs = torch.softmax(last_logits, dim=-1)\n",
    "            predicted_id = torch.argmax(probs).item()\n",
    "\n",
    "            # Append predicted token\n",
    "            generated_tokens.append(predicted_id)\n",
    "            \n",
    "            # Prepare next input\n",
    "            input_ids = torch.tensor([[predicted_id]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Decode full sequence\n",
    "    text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text_gpt(model, tokenizer, prompt, max_new_tokens=20, device=None, temperature=1.2):\n",
    "    if device is None:\n",
    "        device = choose_device()\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Encode prompt\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)  # [1, T]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Forward pass through GPT\n",
    "        logits = model(input_ids)  # shape: [1, seq_len, vocab_size]\n",
    "\n",
    "        # Take the last token logits\n",
    "        logits = logits / temperature\n",
    "        logits = logits[0, -1, :]\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # print(probs.shape)\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1).reshape(1, 1)\n",
    "        \n",
    "        # Greedy decoding (argmax)\n",
    "        # next_token_id = torch.argmax(probs).unsqueeze(0).unsqueeze(0)  # [1,1]\n",
    "\n",
    "        # Append predicted token to sequence\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "    # Decode full sequence\n",
    "    generated_tokens = input_ids[0].tolist()\n",
    "    text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b664f36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.load_state_dict(torch.load(\"saved_models/lstm_final.pt\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27c77bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.load_state_dict(torch.load(\"saved_models/gpt_final.pt\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fca0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Czasem jedno słowo potrafi zmienić cały dzień.\",\n",
    "    \"Wczoraj ktoś zostawił mi kartkę na ławce, bez podpisu.\",\n",
    "    \"No dobra, ale kto w ogóle uznał, że to ma sens?\",\n",
    "    \"To miało być tylko na chwilę, a wyszło jak zawsze.\",\n",
    "    \"Nie wiem, czy to przez pogodę, czy przez ludzi, ale dziś wszystko wydaje się dziwnie ciche.\",\n",
    "    \"„Nie klikaj tam” — powiedział, zanim ekran zgasł.\",\n",
    "    \"W sumie nie planowałem o tym mówić, ale skoro już tu jesteś…\",\n",
    "    \"Dwa dni bez snu i nagle wszystko zaczyna się układać. Ironia, co?\",\n",
    "    \"Kiedy byłem mały, myślałem, że dorośli wszystko wiedzą.\",\n",
    "    \"Czasami po prostu trzeba usiąść, włączyć coś spokojnego i udawać, że świat się nie pali.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af987805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Czasem jedno słowo potrafi zmienić cały dzień . W każdym z nich jest to połączenie , a następnie jedno z dwóch możliwych znaków . W przypadku trzech znaków , które w każdym przypadku są zapisywane jako \" user \" , a nie \" . W przypadku znaków , w których słowo \" user \" oznacza \" stół \" , jest to \" stół \" . Ozzzo – miejscowość i gmina we Włoszech , w regionie Piemont , w prowincji Turyn . Według danych na rok 2004 gminę zamieszkiwało 1101 osób , 15 os . / km² .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_gpt(gpt, tokenizer, prompts[0], max_new_tokens=100, temperature=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-linguistics (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
