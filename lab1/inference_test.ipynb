{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a94e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34d867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM has 21615440 params\n"
     ]
    }
   ],
   "source": [
    "from architectures.lstm import SimpleLSTM\n",
    "from architectures.gpt import GPTDecoder\n",
    "\n",
    "vocab_size = 50_000   # number of tokens\n",
    "embed_dim = 384     # embedding dimension\n",
    "hidden_dim = 384     # LSTM hidden size\n",
    "num_layers = 2\n",
    "\n",
    "lstm = SimpleLSTM(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "param_count = sum(p.numel() for p in lstm.parameters() if p.requires_grad)\n",
    "print(f\"LSTM has {param_count} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6ddb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT has 20690944 params\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50_000\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 2048\n",
    "num_layers = 6\n",
    "context_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "gpt = GPTDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "param_count = sum(p.numel() for p in gpt.parameters())\n",
    "print(f\"GPT has {param_count} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d34a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def choose_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f567b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def measure_inference_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "        print(f\"[{func.__name__}] Inference time: {elapsed:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd480b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F\n",
    "\n",
    "@measure_inference_time\n",
    "@torch.no_grad()\n",
    "def generate_text_lstm(model, tokenizer, prompt, max_new_tokens=20, temperature: float = 1.0, device=None):\n",
    "    if device is None:\n",
    "        device = choose_device()\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    generated_tokens = tokens.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            out, hidden = model(input_ids, hidden)\n",
    "\n",
    "            logits = out[0, -1, :]\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).reshape(1, 1)\n",
    "            input_ids = torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "            generated_tokens.append(next_token_id.item())\n",
    "    \n",
    "    text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "@measure_inference_time\n",
    "@torch.no_grad()\n",
    "def generate_text_gpt(model, tokenizer, prompt, max_new_tokens=20,  temperature=1.0, device=None):\n",
    "    if device is None:\n",
    "        device = choose_device()\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)\n",
    "\n",
    "        logits = logits / temperature\n",
    "        logits = logits[0, -1, :]\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1).reshape(1, 1)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "    generated_tokens = input_ids[0].tolist()\n",
    "    text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b664f36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.load_state_dict(torch.load(\"saved_models/lstm_final.pt\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c77bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.load_state_dict(torch.load(\"saved_models/gpt_final.pt\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fca0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Czasem jedno słowo potrafi zmienić cały dzień.\",\n",
    "    \"Wczoraj ktoś zostawił mi kartkę na ławce, bez podpisu.\",\n",
    "    \"No dobra, ale kto w ogóle uznał, że to ma sens?\",\n",
    "    \"To miało być tylko na chwilę, a wyszło jak zawsze.\",\n",
    "    \"Nie wiem, czy to przez pogodę, czy przez ludzi, ale dziś wszystko wydaje się dziwnie ciche.\",\n",
    "    \"„Nie klikaj tam” — powiedział, zanim ekran zgasł.\",\n",
    "    \"W sumie nie planowałem o tym mówić, ale skoro już tu jesteś…\",\n",
    "    \"Dwa dni bez snu i nagle wszystko zaczyna się układać. Ironia, co?\",\n",
    "    \"Kiedy byłem mały, myślałem, że dorośli wszystko wiedzą.\",\n",
    "    \"Czasami po prostu trzeba usiąść, włączyć coś spokojnego i udawać, że świat się nie pali.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af987805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate_text_gpt] Inference time: 1.2819 seconds\n",
      "Czasem jedno słowo potrafi zmienić cały dzień . Arka Rzeka – akt nadający tytuł\n",
      "podczas w Polsce z 2 lutego 2021 na Międzynarodowym Festiwalu Filmowym w\n",
      "Niemczech . Promujący tematycznie ukazywał się po 2010 roku . Był on tylko\n",
      "miejscem ponad jubileuszu 100 lat . Wykonawcą był do 2000 Olgierd Mens , a\n",
      "dziekanem tejże książki jest Jarosław Lubicz . Następnego dnia droga ( trasa\n",
      "impreza na polskich kat . Q ) jest obiektem trudności w wydobyciu kluczowych\n",
      "technologii komputerowych oraz udostępnień Enigma . W 1997 roku siły MONNP\n",
      "zostały \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.1858 seconds\n",
      "Wczoraj ktoś zostawił mi kartkę na ławce , bez podpisu . Na ich miejscu ppłk\n",
      "Bronisław Bota pobił rekord brunatny . P . C . opowiadał swoje spotkanie 31 :\n",
      "GKS Tychy 5 : 2 z TKH Toruń 3 : 1 do GKS Tychy 2 : 1 i GKS Tychy 2 : 3 , jednak\n",
      "Cracovia z TKH Toruń po raz drugi w Oświęcimiu miała zadecydować o jego życiu 3\n",
      ": 1 . W rewanżu oglądało 2 tormę w bramki i dwoje kibiców , jednak płochtów nie\n",
      "odniósł obrażeń . Spotkanie finałowe obejrzało 45 widzów . W turnieju tym TKH\n",
      "Toruń 3 : 2 \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.1841 seconds\n",
      "No dobra , ale kto w ogóle uznał , że to ma sens ? . Publiczna dumą jest\n",
      "incydent UNIX 2 . godziny przed turniejem od listopada 2019 do kwietnia 2020 .\n",
      "Premier Kanady . MacQuebec uważa się za jeden z najbardziej skorowidzących\n",
      "prezydenckich i ale z pierwszej listy tego typu nie mówił o tym , ile w tych\n",
      "warunkach mam potężnych bohaterów , ale nawet każda , w tym ostatnim przypadku\n",
      "rolę w drafcie agencji BBC One . Somerset Family stwierdził , że społeczność\n",
      "musi ustąpić . W ceremonii UNITA wybiera szereg innych proponowanych wiadomości\n",
      ", dzięki \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.2073 seconds\n",
      "To miało być tylko na chwilę , a wyszło jak zawsze . Następnie to widać na\n",
      "zimnych trasie wycieków na podstawie samochodu . Vonicami Ciarko i Ciarko\n",
      "Tajskie były laptopy w kabinie , a także Apelliga kablowa . Biegania wykonane w\n",
      "tych pomieszczeniach wykorzystujące komputerowa kanał . Powołano pierwsze do\n",
      "użytku na początku 1949 . Prace zostały zrealizowane na początku sezonu , lecz w\n",
      "nim wykonane były braki mechaniczne , a szyby przewieziono do Brazylii i\n",
      "Australii . Niektóre natomiast wszystkich dużych rozmiarów w taki sposób były\n",
      "wyposażone w silnik IBS - 56 \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.2925 seconds\n",
      "Nie wiem , czy to przez pogodę , czy przez ludzi , ale dziś wszystko wydaje się\n",
      "dziwnie ciche . Wolnozmierzchnoże raczej cząsteczki amoniaku niektóre te wektory\n",
      "przedefiniują system z dostępu do roztworu . Jako że enzymy wywołują cechy\n",
      "nieorganiczne oraz biologiczne . Zwykle rozmnażają się jako proces otaczania\n",
      "obiektu do tego znaczenia . Ich tchonozowrzała się po około 24 milimetrów .\n",
      "Większość metaboli jest bólami , co oznacza wiele , które pozwoliło na rybactwo\n",
      ", a nie za odzyskanie narządu . Protechniczne afor . Rak Zdrojki na założenie\n",
      "guber \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.2526 seconds\n",
      "„ Nie klikaj tam ” — powiedział , zanim ekran zgasł . W pierwszym numerze\n",
      "amerykańskiej marki Vorflagge ( książki Lirhard Karpiński o numerze 148 )\n",
      "pojawiły się piosenki z przełomu XVIII / 13 , kiedy to przeprowadzono kolejne i\n",
      "premierowe wywiady , wysyłając ich na ulicę Krzemową , a jako łączniczka „\n",
      "Polonię ” czy „ Kobylanka ” . Polskie kino było w jednym z pierwszych w Polsce ,\n",
      "siedzibą , siedziby , w tym m . in . w budynkach publicznych i 400 mieszkań dla\n",
      "bezrobotnych . Teatr . Adolf Wagner . Następnym projektem było kino „ Bu \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.2604 seconds\n",
      "W sumie nie planowałem o tym mówić , ale skoro już tu jesteś … ” : ) . Stąd\n",
      "myślał też po niemiecku tajni włosiennicy – włoscy odrzucają swoje pierwsze\n",
      "przekonania , jaka mogła mówić przy tej samej fantasy . Ostatecznie opisana\n",
      "praca została sukcesywnie nabierać bardzo stara się mityczna obraz\n",
      "architektoniczny . Autorzy systemu ujmującego się filozofię – w okresie\n",
      "rewolucji francuskiej ludzie w nauczaniu się \" małpą postmodernizmu \" – wówczas\n",
      "amerykański filozof Baptysta Chasnéní , który zainteresował się filozofią i\n",
      "filozofią , pisząc korzystne w całej ha \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.2685 seconds\n",
      "Dwa dni bez snu i nagle wszystko zaczyna się układać . Ironia , co ? KPP ECC .\n",
      "Stanisław Andrzej Kęga ( 1884 – \" Hubert Wojciech Raj \" , 1951 – 2006 ) – polski\n",
      "żołnierz Litwy demokratycznej III dynastii . Życiorys . Jego ojciec był oficerem\n",
      "w 40 pułku kawalerii . Od 1946 członek załogi Stutthof . W latach 1949 – 1952\n",
      "był wiceinspektorem w Armii Krajowej . Po zakończeniu działań wojennych\n",
      "pozostawał w rodzinnym majątku Ploni . Działał w PSZ . W młodości brał udział w\n",
      "koncertach specjalnych kobiet w Gdańsku , a następnie w serialach \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.2190 seconds\n",
      "Kiedy byłem mały , myślałem , że dorośli wszystko wiedzą . Ale car jest\n",
      "wolniejszy od odpowiedzialności za pomoc z HIV i ściągającym ich przez siebie\n",
      "tablicach . Więcławska wykonuje się w niedzielę , gdy Hiszpanie odbiera i\n",
      "biegnie . Liczba nastolatków , 170 w parze z Portugalczykiem jest wzorcem i\n",
      "następstwem dwóch ostatnich dziewięcionastu VIP z obszaru Marsa . Zostaje\n",
      "gwiazdą publiczną , ponieważ będzie on ozdabiony przez szatana . Scarletti\n",
      "zakochuje się w klienta z hotelu odzieżowym . Mimo to Ciara ma w stanie\n",
      "likwidacji , a Wilkowy \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.2715 seconds\n",
      "Czasami po prostu trzeba usiąść , włączyć coś spokojnego i udawać , że świat się\n",
      "nie pali . Tego żeby pią działania i śmierć ludzi do samego ołowic . William\n",
      "Flachmand w maju 1603 roku pozbawiła swych roszczeń do Holsztynu . Wkrótce aż do\n",
      "Prus nad Westem Fryderyk III rozbiły Austriacy . Karl Camille zostało jednak\n",
      "obsadzone przez wojska szwedzkie . W pałacu ogłosiło się ostateczny upadek\n",
      "Szwecji . Stan jego panowania i ochrona . Samorząd Prus miał działanie\n",
      "ujednolicenia stylu i zwyczajów , dlatego w celu osiągnięcia sukcesu w drugiej\n",
      "wojnie w związku z drobnymi sukcesami Szwedów , z których wielu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "for prompt in prompts:\n",
    "    text = generate_text_gpt(gpt, tokenizer, prompt, max_new_tokens=100, temperature=1.0, device=\"cpu\")\n",
    "    print(textwrap.fill(text, width=80), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0dc3c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate_text_lstm] Inference time: 0.2231 seconds\n",
      "Czasem jedno słowo potrafi zmienić cały dzień . Trzy święta obrzeżne są odmienne\n",
      "od małpy . Obecna forma środowiska płynących wynosi 1 , 722 – 100 km i duże\n",
      "znaczenie jako słaba wielkość donora , np . : “ fastyk ” , którego języku\n",
      "sprowadzano Indian z obu stron . PropaBudowlanych mistrza z systemu Los Angeles\n",
      ", głównie z Lindą na czele . Kowal . Poza tym w dziedzinie deklaracje ufności\n",
      "pogorszyło sytuacji – przede wszystkim uprzedzeń lub eliksirów oraz zmiany\n",
      "sprzeczności w znacznie skrótach poruszających równoległymi kamer odbytowanych \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2064 seconds\n",
      "Wczoraj ktoś zostawił mi kartkę na ławce , bez podpisu . Kadrot używa najdłużej\n",
      ", $ dowódca , gdy oboje będą rozpoczynającym się gole . To przyjemność w pełni\n",
      "sprawnej na koniec sezonu . Drużyna musi wejść w radykalni Trójcy w Wardin . Po\n",
      "zrzuceniu , zawodnik , związała się z tym , że odwrócono jeszcze pas mistrzowski\n",
      "w historii futbolu , który z kolei pozostała w federacji Rangers . Gdy\n",
      "reprezentacja 2008 grała bardzo słaba ) spotkania nie miała żadnego turnieju\n",
      "przegrywając . 26 stycznia 2008 trenera piosenkowica Raw podpisał inny rzut w 1\n",
      "sezonie IV ligi ) \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2070 seconds\n",
      "No dobra , ale kto w ogóle uznał , że to ma sens ? Caborcji . Takie były poważne\n",
      "neurosowanie i z podobnym zainteresowaniem zabrali ją zabił Niemiec do\n",
      "obleganego Syrii . Szanse wydawało wypostawienia się w kokpit tego stylu .\n",
      "Francuscy wspierani logiter Mahomet nakręcił dawny powstanie nowojorskiej\n",
      "mniejszości francuskiej w Muzajamie w 1849 roku kolejny we Francji i kierowany\n",
      "przez Christiaki . W 1882 roku wprowadzono nowe znaczenie historyczny ośrodek\n",
      "królestwa , filozofią w latach 1976 - 1976 , wskazując na to , iż „ Honoap \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2042 seconds\n",
      "To miało być tylko na chwilę , a wyszło jak zawsze . Dla swojej piosenki \" The\n",
      "Puboctic Stud Holar McDonald ’ s the Magic World Brooketions \" , nad którym\n",
      "zakoryńskim stała się jego debiutancka solowa płyta obszaru Sunath Berlin , \"\n",
      "Spoto nie wspominali \" został wydany 7 lutego – seria „ Free Amiball ’ s Me Out\n",
      "” zatytułowany \" Qt ' s So Pack \" . Pojęcie zespołu nosiła nazwę Fima\n",
      "Starspowodowany , występuje w japońskiej piosenki XRQC dla wersji 3 . 8 nowej \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2061 seconds\n",
      "Nie wiem , czy to przez pogodę , czy przez ludzi , ale dziś wszystko wydaje się\n",
      "dziwnie ciche . Ze względów termiczne fragmenty świata będzie tajemnicą . Zanim\n",
      "jednak zmienił swą nazwę , przynależy przez przypadek dla wczesnych lat\n",
      "istnienia Starsował do dziś Malam , „ w rzeczywistości ogarnięte korł ” , gdy\n",
      "nie państwo nie traktują żadnego czasu . Zmarł we Lwowie , a montaż zachowano na\n",
      "kilka I obrócić w jeździe owsko - czasznie z wizerunkami złych miejsca kultu .\n",
      "Największą stroną był muwernitalia ( w przypadku sprzecznych z cytryną , każda\n",
      "uśmiechniętą twarz ) . Literatura . \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2066 seconds\n",
      "„ Nie klikaj tam ” — powiedział , zanim ekran zgasł . czynili wykonywali \"\n",
      "Nosowicko - miennie \" . Został też zwolniony z pracy w filmie , roli policjanta\n",
      ", – John już , w rolach głównych kolejno tuż po restauracji Estudiantes . W\n",
      "międzyczasie i Harfy bitwie poddał Ziemię , oba uczelnia czekała pod doskonałość\n",
      "Adolfa Hitlera . Po następnym sezonie został wybrany do gry trzeciej klasy .\n",
      "Przez pięć miesięcy mieszka w miastach i ATP OPL Chad , jaki znów wygrał\n",
      "Griiuligo . Wychowanek Retinger przybyli na wyjazd dwóch kolejnych wyścigów \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2020 seconds\n",
      "W sumie nie planowałem o tym mówić , ale skoro już tu jesteś … pojawili się tu\n",
      "tyłem do nas atutem . Większa grupa kluczowych i węzłów , często o dużym stopniu\n",
      "nowiernym , zajmuje się lożą „ Top ” z męsko - iberyjską . Poza Unią FedeFutbol\n",
      "Austrachistyczna jest jedna piąta jednostka organizuje Partia Obywatelskiego\n",
      "Ruchu Państw Klanu Socjaldemokracjonalnej i ds . specjalnych systemowych metro i\n",
      "uczestnictwo w partii . Po wyborach w 2003 roku Ligi Narodów stwierdzili , iż\n",
      "mocno realizować uchodzący i było szczególnie niebezpieczne . Ruch na rzecz \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2080 seconds\n",
      "Dwa dni bez snu i nagle wszystko zaczyna się układać . Ironia , co ? Mroczne\n",
      "sprawy . Dzwony rzuty już zostały kopiste . Ze wszystkich wierzeń częściowo\n",
      "otrzymał trzykrotnie łocze Magnezja , a na członka Prawa Jana Negaxa oraz\n",
      "Prószyński i S - ka . Punkt konreformatyczną – stał się świadkiem politologii w\n",
      "Petersburgiem i założył w Paryżu festiwal Koncert fortepianowy . Pierwsza z nich\n",
      "znalazła sobie udział w życiu fantastyki kreacjonujących przyjaciół ,\n",
      "wieloletnim wolnym oprogramowaniem . Artysta propagował na polemikę albo badania\n",
      "z czasopismami sympatyz \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2038 seconds\n",
      "Kiedy byłem mały , myślałem , że dorośli wszystko wiedzą . Jeśli są podstępnie\n",
      "ani , bardziej nielepiej przykre napięcie na przedgórze , , mogą być na dosłowne\n",
      ", przezornie chcą podnieść symbol Hisinthmera . Twierdzi on , że zdolny był po\n",
      "prostu stworzyć dialektyczny . Program miał dojść już do Dulonsgard ’ ego 2 ” -\n",
      "' . Według Josephsona Dylana mogą być zarówno dla możliwych radykalnych\n",
      "rozbieżności zgodnych z grupą Hondurasu : Natomiast dla Greków jest teraz „\n",
      "socjalista ” , co jest zarazem socjalistami \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2040 seconds\n",
      "Czasami po prostu trzeba usiąść , włączyć coś spokojnego i udawać , że świat się\n",
      "nie pali . Ale wspina się w ciemności i tył . Ogmania nie zawierają z innych\n",
      "kompozycji . Uczynienie zabawki miała także trwać w tej sytuacji ; przedstawione\n",
      "są również giel „ zgłoski ” , to może próbować ubiegać się o przestępstwo im \"\n",
      "osobie \" , który dzieli się na pełne i poniższe kryteria . \" Święto Jednocedni\n",
      "pełnitwami TVP \" . Pamięć . Od 2004 roku prowadzi nową organizację Jacka Krasha\n",
      "\" Przypadki Polski Reformowania 13 rodzin : 5630 Ewidaina \" Co do \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    text = generate_text_lstm(lstm, tokenizer, prompt, max_new_tokens=100, temperature=1.0, device=\"cpu\")\n",
    "    print(textwrap.fill(text, width=80), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be634c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate_text_lstm] Inference time: 0.2241 seconds\n",
      "W Oceanie Atltantyckim na dnie żyją sobie zarówno odmiana antykomuniorstwo , jak\n",
      "i frakcja wewnętrznej społeczeństwa , a nawet ich aktywistów . Organizowano\n",
      "również trudności z zgromadzeniem i zastraszeniami . Efekty rozbronia . Obie\n",
      "struktury nie były pierwszymi jednostkami ratowniczymi niezawodność służby\n",
      "zdrowia do I wojny światowej . Na obowiązujących zasad piśmienniczych korporacja\n",
      "sytuacji radzieckiej jednostka została ostatecznie ostrzelicana , odsunięta\n",
      "przez Polskę , podobnie jak na konkretne jednostki , w związku z tym w naprawą\n",
      "elektrycznym wybuchł samodzielne rządy społeczne . Nazwy Podziemno - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"W Oceanie Atltantyckim na dnie żyją sobie \"\n",
    "text = generate_text_lstm(lstm, tokenizer, prompt, max_new_tokens=100, temperature=1.0, device=\"cpu\")\n",
    "print(textwrap.fill(text, width=80), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32666ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate_text_lstm] Inference time: 0.2203 seconds\n",
      "Był to rok 1410 , gdy wojska polskie i krzyżackie starły się ze sobą pod\n",
      "Kodomrwem , toteż cały czas ważniejszy od miast w Czechach . Rabin Herzl\n",
      "ponownie przypuszczalnie dla tego miasta płacili jak 12 włoskich srebrnych .\n",
      "Naprzeciw cysternicy z 1411 r . , bo pomimo niemożności Bożego pnia nie została\n",
      "wykonana jednostka pod zamek w 1461 roku , odrzucił jego imię \" Arn \" ( 1731 ) .\n",
      "Uzyskał godność hrabiego Marian Karczmize ze Stomitry Donalda von Seszele . 7\n",
      "czerwca 1185 roku poeta rezygnował \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = generate_text_lstm(lstm, tokenizer, prompt, max_new_tokens=100, temperature=1.0, device=\"cpu\")\n",
    "print(textwrap.fill(text, width=80), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-linguistics (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
