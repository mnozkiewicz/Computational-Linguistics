{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a94e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34d867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM has 21615440 trainable params\n"
     ]
    }
   ],
   "source": [
    "from architectures.lstm import SimpleLSTM\n",
    "from architectures.gpt import GPTDecoder\n",
    "\n",
    "vocab_size = 50_000   # number of tokens\n",
    "embed_dim = 384     # embedding dimension\n",
    "hidden_dim = 384     # LSTM hidden size\n",
    "num_layers = 2\n",
    "\n",
    "lstm = SimpleLSTM(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "param_count = sum(p.numel() for p in lstm.parameters() if p.requires_grad)\n",
    "print(f\"LSTM has {param_count} trainable params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6ddb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT has 20690944 trainable params\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50_000\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 2048\n",
    "num_layers = 6\n",
    "context_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "gpt = GPTDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "param_count = sum(p.numel() for p in gpt.parameters())\n",
    "print(f\"GPT has {param_count} trainable params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d34a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def choose_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f567b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def measure_inference_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "        print(f\"[{func.__name__}] Inference time: {elapsed:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd480b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F\n",
    "\n",
    "@measure_inference_time\n",
    "@torch.no_grad()\n",
    "def generate_text_lstm(model, tokenizer, prompt, max_new_tokens=20, temperature: float = 1.0, device=None):\n",
    "    if device is None:\n",
    "        device = choose_device()\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    generated_tokens = tokens.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            out, hidden = model(input_ids, hidden)\n",
    "\n",
    "            logits = out[0, -1, :]\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).reshape(1, 1)\n",
    "            input_ids = torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "            generated_tokens.append(next_token_id.item())\n",
    "    \n",
    "    text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "@measure_inference_time\n",
    "@torch.no_grad()\n",
    "def generate_text_gpt(model, tokenizer, prompt, max_new_tokens=20,  temperature=1.0, device=None):\n",
    "    if device is None:\n",
    "        device = choose_device()\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)\n",
    "\n",
    "        logits = logits / temperature\n",
    "        logits = logits[0, -1, :]\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1).reshape(1, 1)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "    generated_tokens = input_ids[0].tolist()\n",
    "    text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b664f36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.load_state_dict(torch.load(\"saved_models/lstm_final.pt\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27c77bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.load_state_dict(torch.load(\"saved_models/gpt_final.pt\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fca0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Czasem jedno słowo potrafi zmienić cały dzień.\",\n",
    "    \"Wczoraj ktoś zostawił mi kartkę na ławce, bez podpisu.\",\n",
    "    \"No dobra, ale kto w ogóle uznał, że to ma sens?\",\n",
    "    \"To miało być tylko na chwilę, a wyszło jak zawsze.\",\n",
    "    \"Nie wiem, czy to przez pogodę, czy przez ludzi, ale dziś wszystko wydaje się dziwnie ciche.\",\n",
    "    \"„Nie klikaj tam” — powiedział, zanim ekran zgasł.\",\n",
    "    \"W sumie nie planowałem o tym mówić, ale skoro już tu jesteś…\",\n",
    "    \"Dwa dni bez snu i nagle wszystko zaczyna się układać. Ironia, co?\",\n",
    "    \"Kiedy byłem mały, myślałem, że dorośli wszystko wiedzą.\",\n",
    "    \"Czasami po prostu trzeba usiąść, włączyć coś spokojnego i udawać, że świat się nie pali.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af987805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate_text_gpt] Inference time: 1.2900 seconds\n",
      "Czasem jedno słowo potrafi zmienić cały dzień . Oficjalna strona na język\n",
      "niemieckim trafiło na swoje perskie określenie , ale były używane w języku\n",
      "rosyjskim . Jeśli ktokolwiek dowodził , byli powracający do Port Royal : ff Nowy\n",
      "Jork ” ( w Wenezueli wygrana 1 : 2 ) . Masy ten dotyczy formalnie zaradłego\n",
      "narodu w Polsce . Piraci Urugwaj brali również udział w wielu rozgrywkach , m .\n",
      "in . w roku 1976 po raz pierwszy w historii jako autor nu , który zajmuje mniej\n",
      "swoich potencjalnych kibiców . Na arenie międzynarodowej flaga argentyńska\n",
      "wywoziona \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.3211 seconds\n",
      "Wczoraj ktoś zostawił mi kartkę na ławce , bez podpisu . W dawnej fy komiks \"\n",
      "pioruny u wikaczy , tak więc Jakub został obaj zagwarantowany trzem jednostkom\n",
      "niemieckiej policji , która korzystała z niej wielu oficerów . EifflenHugo\n",
      "Essenberg ( ur . 3 kwietnia 1932 w Lünestann , zm . 3 grudnia 2019 w Moskwie ) –\n",
      "belgijski reżyser filmowy , teatralny i telewizyjny , wieloletni twórca\n",
      "produkcji filmowej . Studiował w kilka modelarskich kinach oraz tydzień po\n",
      "premierze kilku filmów , w tym także w języku czeskim . \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.2205 seconds\n",
      "No dobra , ale kto w ogóle uznał , że to ma sens ? . \" Sprawę rozstał się кk \" .\n",
      "Dlatego Bóg zaporównują mila z Ziemię , jak kilka osób udaje . Naзkiem się do\n",
      "cichu proroków . Jednak w świetle przekonania , że Bóg rozpoczynał skarbiec w\n",
      "spotkaniu . Wszyscy Polujcie z Wwinku uręką , trafiają do rzeki Dahby i wdaje\n",
      "się z niej w sól , zaś Ostatecznie towarzyszyli pan . Bohaterowie - Lucy nie\n",
      "towarzyszy nam go więc . Makbeta jest też druga . Anglia jedzą się po \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.1961 seconds\n",
      "To miało być tylko na chwilę , a wyszło jak zawsze . 11 marca koreańscy ubiegłsi\n",
      "piloci okrętów Oil próbowali zatrzymał ogień z Krety . To nadeszła latem , 26\n",
      "września doszciła się do innych błędów i ciemności . W lot ataki niemieckie były\n",
      "rozstrzygające , że spodziewano się dużego wysiłku bojowego z uwagi na wielki\n",
      "jafeld utrudniało obchodzenie od pancerników . W tym samym czasie , dnia 9\n",
      "września stelomeracja honosi odmówiła wsparcia artylerii i piecliwych artylerii\n",
      ", dając powżony szkielet , który po jej zakończeniu nigdy \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.1811 seconds\n",
      "Nie wiem , czy to przez pogodę , czy przez ludzi , ale dziś wszystko wydaje się\n",
      "dziwnie ciche . Piekryje imiona bogate były w muzykę ze graniczącymi ze swoją\n",
      "kulturystyką i atrakcyjnością . Jej imiona są odwrotne : \" PN - u ! \" . Nieco\n",
      "większy o nieznanej z tworzyw sztucznych satelitarnych – nazwa wolna „\n",
      "współczesna ” oznaczająca krzywa , koloru bądź zajęcia i wartości ścisłych nici\n",
      ". W związku z tym przyjęło się hipotezę PGC 1102 , która „ istnieje tylko w\n",
      "systemie administracji ” upowszechniła działalność zespołu i prowadziła prace\n",
      "zatwierdzana w 1957 przez wieniec i co potwierdziło ich \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.0898 seconds\n",
      "„ Nie klikaj tam ” — powiedział , zanim ekran zgasł . „ Podczas niowski ” fakt ,\n",
      "że „ ten | był czasem ” . Metologia . Jest to forma odwrotna dla melodii i\n",
      "czystego standardu Jana Mertona . Bywa to jednak , że wiązanie zbliżone było do\n",
      "autora powtórzeń wytwórców inicjacji . ( \" Pukan Set hen \" ) co następuje w\n",
      "pierwszym fragmencie pisanego połączonego \" d \" - o \" ( „ stica ” , \" biegnie\n",
      "inaczej w niewoli niemieckiej , zdeformowanej niemal do kilkuset października\n",
      "1988 ) . W pierwszej połowie \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.1499 seconds\n",
      "W sumie nie planowałem o tym mówić , ale skoro już tu jesteś … \" ) . W 2009 roku\n",
      "opublikowana została sprzedaż artykułu w serwisie internetowym YouTube . Według\n",
      "opracowania \" Nasze SAZ , poprzedniego ( 2013 ) i dwóch różnorakich materiałów\n",
      "niezaliczowych doprowadziły jej do zmiany wizerunku . W 2016 ukazała się\n",
      "kompilacja DVD zawierające toksyny materiały licząca 252 , 7 gp ( \" Faxie\n",
      "Detweirement \" ) o emisji 1 , 6 R , 8 KERS ( \" Baryate Live \" , 2019 ) . \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.2364 seconds\n",
      "Dwa dni bez snu i nagle wszystko zaczyna się układać . Ironia , co ? ? [ Android\n",
      "1 : 3 ] 1952 . 25 września 1955 papież Juliusz II przebywał w Jerozolimie 21\n",
      "października , kiedy został obalony patriarcha Konstantynopola . 1 czerwca 1956\n",
      "w Jerozolimie wzięli udział członkowie typu grecki wikariusz , ksiądz Bibulza .\n",
      "Naczelny Sąd Administracyjny uznał też zwierzchnictwo nad Palestyną i\n",
      "Palestyńczykami , bądź podczas których Władysław II był w ręce przewodniczącego\n",
      "Kolegium Kardynałów . Odznaczony Krzyżem Srebrnym ( raniony przede wszystkim\n",
      "przez papieża ) Krzyżem Niepodległości . W styczniu 1974 jednym z \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.2008 seconds\n",
      "Kiedy byłem mały , myślałem , że dorośli wszystko wiedzą . Teraz należy zemścić\n",
      "się dla siebie , jeżeli można dojść do jego wnętrza , Hossi , ponieważ on pod\n",
      "groźbą kary odmawia dany teren nienawiści ? Czy A mienia nie dzi się ze swojej\n",
      "żony ? ” . Bossi zostaje reżyserem niektórych Eforyów . Wzburzone jest chwile :\n",
      "ukończenie kinowe rekompensuje właśnie rowa dyskoteka . ( Bóg zawiadomił , że\n",
      "nie skręci na przystankach ú pl . A . dem ) . Według Colla odbiera gniazdo\n",
      "PaFack na rozmowę szczęścia . \n",
      "\n",
      "[generate_text_gpt] Inference time: 1.1815 seconds\n",
      "Czasami po prostu trzeba usiąść , włączyć coś spokojnego i udawać , że świat się\n",
      "nie pali . Czarnecki jednak w celu odbicia zdarzenia zwraca uwagę zatem do niego\n",
      "Sikorski także w swojej późniejszego , przechodni pomijając co wieczorem .\n",
      "Chrześcijanie poprosił właściciela o nie stać . Wolność niewłaściwe naprawdę\n",
      "zachowają filmowcy i bajeczni , aby najgorsza naszego kraju , a nie film żaden z\n",
      "nich autor . Lucy musi dalej zobaczyć coś , co nikomu nie zobaczy . Lahaczach\n",
      "tajemnic Buddy , w wymiarze Księżyca jest . Inne inne zdanie powiedziano :\n",
      "contrami ( SNKT ) : słowy , pokonuje \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "for prompt in prompts:\n",
    "    text = generate_text_gpt(gpt, tokenizer, prompt, max_new_tokens=100, temperature=1.0, device=\"cpu\")\n",
    "    print(textwrap.fill(text, width=80), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0dc3c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate_text_lstm] Inference time: 0.2261 seconds\n",
      "Czasem jedno słowo potrafi zmienić cały dzień . Trybuny ekonomiczne starają się\n",
      "unikać , aby wymiar sprawiedliwości , trujący się zdozna do wykonywania ataków\n",
      "na cele wydawnicze , np . przy jednoczesnym zachowaniu wykształcenia\n",
      "stomatologii , nauczyciele , którzy prześladowali jakiekolwiek włoskie i\n",
      "kulturowe fyllekspitów oraz chcąc wrócić do życia , pierwszym i w efekcie\n",
      "doradzał antyrządowi tiony ciesiewczego przy operacji negocjacji , którą\n",
      "działalność jest ostatecznie tyczszy , a efekt nowego neutronu . W dobie\n",
      "stalinizmu . Tymczasem Metałocito rozum . W Polsce \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2104 seconds\n",
      "Wczoraj ktoś zostawił mi kartkę na ławce , bez podpisu . Latem 2006 przy\n",
      "szpitalu tego kursu w walce żyją o jeden rok , w \" Włostmuntowni jach \" pisano\n",
      "na własnym boisku pierwszą dyszę . W grudniu 2019 wstąpiła do czołowych krajów\n",
      "socjalistycznych z Mściwoj I . W 1997 działacze Barbary Sośnicko wniosek Jaklubu\n",
      "Anatyjskiego Częstochowa otrzymali na jednej z komunistów w negocjacjach z\n",
      "całkowitym współautoborami 36 samodzielnych prac zastrzeżonych w którymś\n",
      "momencie , w dalszej sytuacji awaryjnej , terrorymy i ucieczka z rolnictwa .\n",
      "Władze podejmowali próby klasy \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2062 seconds\n",
      "No dobra , ale kto w ogóle uznał , że to ma sens ? Skáalajaa za sztukę .\n",
      "Sędziowie musiał opowiedzieć się nie na pewnym przyjście w sąsiedniej Bryii .\n",
      "Ona , gdyż teraz nawet widząc , że Triglaon wystąpił jako rezerwowy , a na\n",
      "prawym skrzydle zamordował pierwszy rząd duńskiego miksta . 19 kwietnia 1989\n",
      "nowym pit - stoczeniem , weźmie udział w wyprzedaży rowerem Anderson razem z\n",
      "Maksem Vectra . Statystyki . Estudiantes borykał się z futbolem dla pierwszej\n",
      "ligi – zespołów Estudiantes z Vest \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2032 seconds\n",
      "To miało być tylko na chwilę , a wyszło jak zawsze . Na zachód Ketula , w\n",
      "wyzwolonych piórach , stka a da wosk . 2 rok później trafia do trzech binset do\n",
      "mostu . Jego pomnika nad wejściem znajduje się w najkrwawszej generacjach w\n",
      "krypcie archiwalnych scen . Opracowano również 17 września 1970 roku ponownie ,\n",
      "„ kochankowie i 25 prowincjała ” . Życie prywatne . Jest synem m . in . z którą\n",
      "miała zdobyć siedem lat . Rodzina Hartwig miało miejsce 30 września 1961 .\n",
      "uchodźców ( THO \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2047 seconds\n",
      "Nie wiem , czy to przez pogodę , czy przez ludzi , ale dziś wszystko wydaje się\n",
      "dziwnie ciche . Jednakże wolnomularz , wróżby przez stegozaura , klinem , pstry\n",
      "etiologią , grofiną , są : społecznej wpływowi od zasług kraty . Skazał dwa\n",
      "długie okresy na dodatek sam , gdyż jest często potomstwem Klinych . W \" Hymmach\n",
      "i Rell Gabrielle \" jest akademickiego słynnego A - dur i srebrny w 1936 r .\n",
      "Przewodniczył ją z Towarzystwem Republiki . Nasz Dom dla międzykulturowego i\n",
      "publicznego na Hollywood przyniósł jej znakomitego miejsce \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2074 seconds\n",
      "„ Nie klikaj tam ” — powiedział , zanim ekran zgasł . wznać w sposób że to\n",
      "słabszony uśmiech . 22 czerwca 2010 w trakcie pierwszego serialu animowanego \"\n",
      "Gwiezdnych wojen \" . Premierowa nazwa konik brytyjskich zdecydowała się na\n",
      "kolejną sesję o galaremę dwukierunkowym \" Agugad \" w wygranym 1 : 0 i właśnie z\n",
      "Danielem Hamiltonem , w której ostatecznie Prowadzący przejęła Jacka Broa .\n",
      "Płyta nie została ustalona . W maju 2005 zespół wystąpił na albumie pt . \"\n",
      "Ponagar basista \" w marcu 2009 roku ; w \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2037 seconds\n",
      "W sumie nie planowałem o tym mówić , ale skoro już tu jesteś … ” poetycka gitarę\n",
      "po powstaniu . Punkt Ogrodu Kaliny ( ) – próbę ścignięcia z 100 cząstek formula\n",
      "_ 4 Stąd formula _ 2 jest metryką liczby 1 . W taki sam efekt układ fizyczny ,\n",
      "więc formula _ 2 jest liczbą wektorową interpretowaną przez przemienne odnóży\n",
      "układu . Przechowywem właściwości namagnesowania Likazu . Została zbudowana\n",
      "przez niego macierzowego w postaci długoałego składowych ( Demeter Sugar SReiser\n",
      ") lub co \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2047 seconds\n",
      "Dwa dni bez snu i nagle wszystko zaczyna się układać . Ironia , co ? Łącznie .\n",
      "Gra pochodzi z mocnymi bośniackimi , melodiami a alkanami . Wymieranie krtani\n",
      "wygląda spóklaczy czarny . Grzegorza i dużo woltoczkiem szałczny jest pedagog\n",
      "rzemiosła . UchesUK także chłopów . Do 1914 r . z zawodu garnizonor , prowadzona\n",
      "przez niego narzędziami dawni robotnicy rolni . Historia . Przemowa powieść \"\n",
      "Olga była dążeniem do zbawienia po śmierci \" . Na pustkach , bpa liturgicznego ,\n",
      "wieś została zamieniona na czworobo \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2056 seconds\n",
      "Kiedy byłem mały , myślałem , że dorośli wszystko wiedzą . : Walerę Obojim i\n",
      "Krótkotrutny z „ odkrył pytanie AO ” . Leconte mu quecy , która odmówiła\n",
      "kwalifikacji lub specjalnych Żołnierze Girej złablalnych i „ flamen ” musiał\n",
      "zbiegł się z okrętami sabotażowymi z następnymi stanów , a więc pojawiały się\n",
      "takie , wpływu jak zamachy , wywiady i ruchy wokół twierdzenia . Oryginalnie nad\n",
      "najważniejszym atakiem do ataku grupa pomiędzy wizytę Williama Celemaino\n",
      "Wemenhotepa zaczął wszcząć niezmienne \n",
      "\n",
      "[generate_text_lstm] Inference time: 0.2047 seconds\n",
      "Czasami po prostu trzeba usiąść , włączyć coś spokojnego i udawać , że świat się\n",
      "nie pali . Powiedział , , że chce , aby znak przekazu zastąpił go w szaleńcu\n",
      "zgraną przeciwosłowacką próbę , a Quidam , w tym bursnelskim Cuellano nie mógł\n",
      "na niej strzelać . Łącznie adoptował 5 tysięcy żołnierzy , co jednak wcześniej\n",
      "prowadziła jazdę arabską . W marcu i sierpniu 1919 - 1945 podryciła ich jej\n",
      "proedukacja . Nie udało się już rozwiązać tego zwrotu . O ile jeden legion –\n",
      "koncert , który zaciągnął się o to podpisać oddział Naczelnik Evesa . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    text = generate_text_lstm(lstm, tokenizer, prompt, max_new_tokens=100, temperature=1.0, device=\"cpu\")\n",
    "    print(textwrap.fill(text, width=80), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-linguistics (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
